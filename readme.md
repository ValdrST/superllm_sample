# SuperLLM Sample (CPU Edition)

This project provides a local LLM inference service using `llama-cpp-python` running **entirely inside Docker**, using **CPU only**.  
It is compatible with **Windows**, **Linux**, and **macOS**, as long as Docker Desktop is installed.

## Requirements

- Docker Desktop (Windows / Linux / macOS)
- Docker Compose v2
- At least **8 GB RAM**
- A GGUF model placed in the `models/` directory

## Project Structure

```
â”‚   config.json
â”‚   docker-compose.yml
â”‚   Dockerfile
â”‚   pyproject.toml
â”‚   readme.md
â”‚
â”œâ”€â”€â”€log
â”‚       superllm_sample.log
â”‚
â”œâ”€â”€â”€model
â”‚       qwen2.5-0.5b-instruct-q4_0.gguf
â”‚
â”œâ”€â”€â”€output
â”œâ”€â”€â”€postgres
â”‚       Dockerfile
â”‚
â””â”€â”€â”€src
    â”‚   readme.md
    â”‚   requirements.txt
    â”‚   setup.py
    â”‚
    â”œâ”€â”€â”€cache
    â”œâ”€â”€â”€live
    â”œâ”€â”€â”€log
    â”œâ”€â”€â”€model
    â”œâ”€â”€â”€output
    â””â”€â”€â”€superllm_sample
        â”‚   main.py
        â”‚   wsgi.py
        â”‚   __init__.py
        â”‚
        â”œâ”€â”€â”€core
        â”‚       Crawler.py
        â”‚       Server.py
        â”‚       SQLResolver.py
        â”‚       __init__.py
        â”‚
        â”œâ”€â”€â”€fetch
        â”‚       FetchPostHolder.py
        â”‚       __init__.py
        â”‚
        â”œâ”€â”€â”€models
        â”‚       Qwen.py
        â”‚       __init__.py
        â”‚
        â””â”€â”€â”€pipeline
                Pipeline.py
                __init__.py
```

## Model Setup

Place your `.gguf` model into `models/`, for example:

```
models/model.gguf
```

Recommended CPU models:
- LLaMA-3.1 8B Instruct (Q4_K_M)
- Mistral-7B v0.3 Instruct (Q4_K_M)

## Running with Docker Compose

### 1. Build the image

```
docker-compose --build
```

### 2. Start the service

```
docker-compose up
```

The API will be available at:

```
http://localhost:3003
```

To run in the background:

```
docker compose up -d
```

To stop:

```
docker-compose down
```

## Internal Architecture

```
Docker Container
â”‚
â”œâ”€â”€ Flask Server
â”œâ”€â”€ llama-cpp-python (CPU mode)
â””â”€â”€ GGUF Model
```

## Environment Variables

| Variable     | Description                     | Example              |
|--------------|---------------------------------|----------------------|
| MODEL_PATH   | Path to the GGUF model          | /models/model.gguf   |
| N_THREADS    | Number of CPU threads           | 4                    |
| MAX_TOKENS   | Max output tokens               | 2048                 |
| HOST         | Bind address                    | 0.0.0.0              |
| PORT         | API port                        | 3003                 |


## Performance Tips (CPU Only)

- Use quantized models (Q4_K_M)
- Increase CPU threads in docker-compose.yml
- Prefer models < 8B for laptops or small servers

## Cleaning Up

```
docker compose down
docker rmi superllm_sample
```


## Running & Testing the Pipeline

Once the containers are up, you can run and test the pipeline using the following endpoints:

### â–¶ï¸ Run the Pipeline
Trigger the full ETL + LLM enrichment pipeline:

```
http://localhost:3002/run_pipeline
```

### ðŸ“¥ Fetch Processed Results
Retrieve enriched and validated records.  
For example, to fetch the record with ID=10:

```
http://localhost:3002/posts/10
```

## Credits

Built with:
- llama-cpp-python
- Python 3
- Docker
- PostgreSQL
